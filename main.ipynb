{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "puZgvM8r0aJx",
    "outputId": "ebdb2225-aa01-4a30-9994-e9c186c69751",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-03-28T21:29:07.974342Z",
     "start_time": "2024-03-28T21:29:07.858237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mesa/Coding Projects/LLM-Debug-Assistant/venv/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "q2oqmolk0aJy",
    "outputId": "cb64edb7-c4ab-463a-b885-af80fe45d534",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-03-29T01:52:39.328324Z",
     "start_time": "2024-03-29T01:52:39.324468Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6Oi4Ha2-0aJy",
    "ExecuteTime": {
     "end_time": "2024-03-29T01:52:40.995550Z",
     "start_time": "2024-03-29T01:52:40.992696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# New instruction dataset\n",
    "stack_overflow_dataset = \"Mxode/StackOverflow-QA-C-Language-40k\"\n",
    "# other datasets\n",
    "\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = \"llama-2-7b-chat-stack-overflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wai6lzT70aJy",
    "outputId": "5091ae8d-a46e-4a48-d04e-2ddf1c5c478e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-03-29T01:52:49.181150Z",
     "start_time": "2024-03-29T01:52:44.013242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/893 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4de36c2ed38a423881756039553a5700"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 29.9M/29.9M [00:02<00:00, 11.4MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9ded95de77947e69e803f1e1d48d349"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(stack_overflow_dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-TEmF43d0aJy",
    "ExecuteTime": {
     "end_time": "2024-03-29T01:52:56.810637Z",
     "start_time": "2024-03-29T01:52:55.907172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/40649 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f87c8d9df444f4d85372861d19db36a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset currently has 'question' and 'answer' columns. We need to combine them into a single column named 'text'\n",
    "dataset = dataset.map(\n",
    "    lambda examples: {\n",
    "        \"text\": \"<s>[INST]\" + examples[\"question\"] + \"[\\INST] \" + examples[\"answer\"] + \"</s>\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7-ZugjQP0aJy",
    "ExecuteTime": {
     "end_time": "2024-03-29T01:52:58.231893Z",
     "start_time": "2024-03-29T01:52:58.224366Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9mYfPPTa0aJy",
    "outputId": "ba5c8d93-38a6-40f6-e3ea-be54080b6a07",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "ExecuteTime": {
     "end_time": "2024-03-29T01:53:20.846097Z",
     "start_time": "2024-03-29T01:53:01.850894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd1a39c874584b5f86a52badba9f7b0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mesa/Coding Projects/LLM-Debug-Assistant/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Users/mesa/Coding Projects/LLM-Debug-Assistant/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Users/mesa/Coding Projects/LLM-Debug-Assistant/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/mesa/Coding Projects/LLM-Debug-Assistant/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!pip install accelerate\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float16,\n",
    "    #low_cpu_mem_usage=True,\n",
    "    # quantization_config=quant_config,\n",
    "    # device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "RiWsACS60aJy",
    "ExecuteTime": {
     "end_time": "2024-03-29T01:53:24.906510Z",
     "start_time": "2024-03-29T01:53:24.742450Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "3AmfTzga0aJy",
    "ExecuteTime": {
     "end_time": "2024-03-29T01:53:26.742631Z",
     "start_time": "2024-03-29T01:53:26.739224Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NvQ71iJx0aJy",
    "ExecuteTime": {
     "end_time": "2024-03-29T01:53:27.241432Z",
     "start_time": "2024-03-29T01:53:27.237507Z"
    }
   },
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VWwPLdTU0aJy",
    "ExecuteTime": {
     "end_time": "2024-03-29T01:54:47.744611Z",
     "start_time": "2024-03-29T01:53:29.293164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mesa/Coding Projects/LLM-Debug-Assistant/venv/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/40649 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9640e6ee2ad41bb8c2da2e3feef5819"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mesa/Coding Projects/LLM-Debug-Assistant/venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6Vtheh0T0aJz",
    "ExecuteTime": {
     "end_time": "2024-03-29T01:54:48.034857Z",
     "start_time": "2024-03-29T01:54:47.746385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('llama-2-7b-chat-stack-overflow/tokenizer_config.json',\n 'llama-2-7b-chat-stack-overflow/special_tokens_map.json',\n 'llama-2-7b-chat-stack-overflow/tokenizer.json')"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets compare asking 3 questions to our fine tuned model vs our base model\n",
    "\n",
    "## fine tuned model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "8GCp2ck90aJz",
    "ExecuteTime": {
     "end_time": "2024-03-29T02:31:57.376534Z",
     "start_time": "2024-03-29T01:55:02.503498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I loop through an array in C? [/INST]  In C, you can loop through an array using a `for` loop or a `while` loop. Here are the basic syntaxes for each:\n",
      "\n",
      "For loop:\n",
      "```\n",
      "for (int i = 0; i < array_size; i++) {\n",
      "    // loop body\n",
      "}\n",
      "```\n",
      "In this syntax, `array_size` is the number of elements in the array, and `i` is the index of the current element.\n",
      "\n",
      "While loop:\n",
      "```\n",
      "int i = 0;\n",
      "while (i < array_size) {\n",
      "    // loop body\n",
      "    i++;\n",
      "}\n",
      "```\n",
      "In this syntax, `i` is initialized to 0, and the loop continues until `i` is greater than or equal to `array_size`.\n",
      "\n",
      "You can also use a `for_each` function in\n"
     ]
    }
   ],
   "source": [
    "#logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"How do I loop through an array in C?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yQr1lB4l0aJz",
    "ExecuteTime": {
     "end_time": "2024-03-29T03:14:25.220922Z",
     "start_time": "2024-03-29T02:31:57.379294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What does the 'sizeof' operator do in C? [/INST]  In C, the `sizeof` operator is used to determine the size of a variable or an object in memory.\n",
      "\n",
      "The `sizeof` operator takes a single argument, which can be a variable, a type, or a constant expression. The operator returns the size of the object or variable in bytes.\n",
      "\n",
      "Here are some examples of how to use the `sizeof` operator in C:\n",
      "\n",
      "1. To get the size of a variable:\n",
      "```\n",
      "int x = 5;\n",
      "printf(\"The size of x is %zu\\n\", sizeof(x)); // Output: The size of x is 4\n",
      "```\n",
      "In this example, `x` is an `int` variable with a size of 4 bytes.\n",
      "\n",
      "2. To get the size of a type:\n",
      "```\n",
      "struct Person {\n",
      "    int age;\n",
      "    char name[\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What does the 'sizeof' operator do in C?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "6TGwFAM90aJz",
    "ExecuteTime": {
     "end_time": "2024-03-29T03:45:27.860589Z",
     "start_time": "2024-03-29T03:14:25.222418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] I'm working on a C project in Visual Studio Code and I would like to use GNU formatting in the project. Is there any extension to achieve this or any way to go about it?? [/INST]  Yes, there are several extensions available in Visual Studio Code that can help you format your code using GNU formatting rules. Here are a few options you can consider:\n",
      "\n",
      "1. GNU Formatter Extension: This extension provides a simple way to format your code using GNU formatting rules. You can install it by searching for \"GNU Formatter\" in the Visual Studio Code Marketplace. Once installed, you can use the \"GNU Formatter\" command in the Command Palette (Ctrl + Shift + P on Windows or Command + Shift + P on Mac) to format your code.\n",
      "2. Prettier: Prettier is a popular code formatter that supports a wide range of programming languages, including C. You can install\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I'm working on a C project in Visual Studio Code and I would like to use GNU formatting in the project. Is there any extension to achieve this or any way to go about it??\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## base model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4476c75e094c4f56b210946ec7a66951"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I loop through an array in C? [/INST]  In C, you can loop through an array using a `for` loop or a `while` loop. Here are the basic syntaxes for each:\n",
      "\n",
      "For loop:\n",
      "```\n",
      "for (int i = 0; i < array_size; i++) {\n",
      "    // loop body\n",
      "}\n",
      "```\n",
      "In this syntax, `array_size` is the number of elements in the array, and `i` is the index of the current element.\n",
      "\n",
      "While loop:\n",
      "```\n",
      "int i = 0;\n",
      "while (i < array_size) {\n",
      "    // loop body\n",
      "    i++;\n",
      "}\n",
      "```\n",
      "In this syntax, `i` is initialized to 0, and the loop continues until `i` is greater than or equal to `array_size`.\n",
      "\n",
      "You can also use a `for_each` function in\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"How do I loop through an array in C?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T03:48:12.846379Z",
     "start_time": "2024-03-29T03:45:27.862911Z"
    }
   },
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bab5202504b84264b3aa65df791e1159"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What does the 'sizeof' operator do in C? [/INST]  In C, the `sizeof` operator is used to determine the size of a variable or an object in memory.\n",
      "\n",
      "The `sizeof` operator takes a single argument, which can be a variable, a type, or a constant expression. The operator returns the size of the object or variable in bytes.\n",
      "\n",
      "Here are some examples of how to use the `sizeof` operator in C:\n",
      "\n",
      "1. To get the size of a variable:\n",
      "```\n",
      "int x = 5;\n",
      "printf(\"The size of x is %zu\\n\", sizeof(x)); // Output: The size of x is 4\n",
      "```\n",
      "In this example, `x` is an integer variable with a size of 4 bytes.\n",
      "\n",
      "2. To get the size of a type:\n",
      "```\n",
      "struct Person {\n",
      "    int age;\n",
      "    char name[20\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What does the 'sizeof' operator do in C?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T03:55:30.042212Z",
     "start_time": "2024-03-29T03:48:12.848689Z"
    }
   },
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5306882070744664834fabbae283d6c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] I'm working on a C project in Visual Studio Code and I would like to use GNU formatting in the project. Is there any extension to achieve this or any way to go about it?? [/INST]  Yes, there are several extensions available in Visual Studio Code that can help you format your code using GNU formatting rules. Here are a few options:\n",
      "\n",
      "1. GNU Formatter Extension: This extension provides a simple way to format your code using GNU formatting rules. You can install it by searching for \"GNU Formatter\" in the Visual Studio Code Marketplace.\n",
      "2. Prettier: Prettier is a popular code formatter that supports a wide range of programming languages, including C. You can install it by searching for \"Prettier\" in the Visual Studio Code Marketplace.\n",
      "3. Code Beautifier: Code Beautifier is another extension that can help you format your code using GNU formatting rules. You can install it\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I'm working on a C project in Visual Studio Code and I would like to use GNU formatting in the project. Is there any extension to achieve this or any way to go about it??\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T03:59:23.603027Z",
     "start_time": "2024-03-29T03:55:30.054011Z"
    }
   },
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "These questions show that while the pretrained model is already quite smart, it lacks some knowledge provided by our dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n          const url = new URL(\"/\", window.location);\n          const port = 4000;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "log_dir = \"results/runs\"\n",
    "notebook.start(\"--logdir {} --port 4000\".format(log_dir))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T01:48:39.660511Z",
     "start_time": "2024-03-29T01:48:37.788708Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
